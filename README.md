# Analyze
On Intel CPU system we have fastest inference with yolov8n model exported to OpenVINO format.

# TODO

* Add warmup (5 frames) before benchmarked
* Analaze OpenVINO int8 model
* Check custom trained model
* Write min/max warmup
* Write min/max inference
* Write after inference model to csv file
* Write model size